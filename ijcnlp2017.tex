%\title{ijcnlp 2017 instructions}
% File ijcnlp2017.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{ijcnlp2017}
\usepackage{times}
\usepackage{latexsym}

\usepackage{bm} % vec bf
\usepackage[dvipdfmx]{graphicx} %graph  
\usepackage{amsmath, amssymb} % for R   
\usepackage{booktabs} % midrule
\usepackage{amsmath } %|
\usepackage{url}
\usepackage{amsmath}

% Uncomment this line for the final submission:
\ijcnlpfinalcopy

%  Enter the IJCNLP Paper ID here:
\def\ijcnlppaperid{***}

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

\newcommand\BibTeX{B{\sc ib}\TeX}


\title{Long Short-Term Memory for Japanese Word Segmentation.}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
\author{Derek Wong\and Lung-Hao Lee \\
  {\tt publication@ijcnlp2017.org}}

\date{}

\begin{document}
\maketitle

\setlength{\abovedisplayskip}{2pt} % 上部のマージン
\setlength{\belowdisplayskip}{2pt} % 下部のマージン

\begin{abstract}
This study presents a Long Short-Term Memory (LSTM) neural network approach to Japanese word segmentation (JWS). 
Previous studies on Chinese word segmentation (CWS) succeeded in using recurrent neural networks such as LSTM and gated recurrent units (GRU). 
However, in contrast to Chinese, Japanese includes several character types, such as hiragana, katakana, and kanji, that produce orthographic variations and increase the difficulty of word segmentation. 
Additionally, it is important for JWS tasks to consider a global context and yet, traditional JWS approaches rely on local features.
In order to address this problem, this study proposes employing an LSTM-based approach to JWS. 
The experimental results indicate that the proposed model achieves state-of-the-art accuracy with respect to various corpora. 
%Since we first introduce neural network approach for JWS, we investigate some neural network architecture and analyze the characteristic by comparing with current state-of-the-art method on several corpora that have different nature.

\end{abstract}

\section{Introduction}
Word segmentation is a fundamental task in Japanese language processing. 
%It is common to describe word segmentation and POS tagging as word segmentation in Japanese.
Especially, errors in word segmentation in East Asian languages, such as Japanese and Chinese, that lack a trivial word segmentation process can cause problems for downstream NLP applications.
Thus, it is crucial to perform accurate word segmentation for Japanese NLP tasks.
%The current popular JWS is based on sequence labeling problem~\cite{neubig-nakata-mori:2011:ACL-HLT2011}. 


In order to achieve high accuracy, almost all modern methods in JWS utilize discriminative models with extensive feature engineering.
However, machine learning-based methods tend to require hand-crafted feature templates and suffer from data sparseness.
Neural network models have been investigated for various NLP tasks to address the problem of feature engineering, \cite{liu2015multi,sutskever2014sequence,socher2013parsing,turian2010word,mikolov2013distributed}.
Neural network models enable the use of dense feature vectors termed as embeddings learned through representation learning~\cite{NIPS20135021}. 

Another important problem in JWS corresponds to context modeling. Traditional JWS methods employ feature templates to expand local features in a fixed window. However, global information beyond the window is not considered. Conversely, recurrent neural network models grasp long distance information due to Long Short-Term Memory (LSTM) and record state-of-the-art accuracy in Chinese word segmentation \cite{chen-EtAl:2015:EMNLP2}. 
However, it is not clear as to whether the LSTM approach is also effective in JWS since there are many types of character sets in Japanese that produce orthographic variations. 

Therefore, a LSTM network is proposed for Japanese word segmentation by incorporating character-level embeddings and long distance dependency.
The main contributions of this study are as follows:

\begin{itemize}

\item An LSTM model is proposed for JWS and methods to utilize sparse features, such as character type, character N-gram, and dictionary features, are investigated. %To our knowledge, this is the first attempt to apply recurrent neural networks to JMA task.

\item The experimental results indicate that the word segment model achieves state-of-the-art performance in both token-level and sentence-level accuracy with respect to various datasets. 

%\item Experimental results show that our POS tagging model achieves state-of-the-art performance in both token-level and sentence-level accuracy on various datasets.
%We also analyze our neural network approach in various corpus to find the characteristics of neural network model.
%We evaluate our method on the web text to examine the potential to solve out-standing problem of Japanese word segmenation.


%Our proposed model is competitive with state-of-the-art method, despite it still has room for improvement. Although neural network approach tend to be time consuming, it is useful when the accuracy is required.  

\end{itemize}

\section{LSTM for Japanese word segmentation}
%We will explain Japanese word segmentation using LSTM.
%First, we explain the word segmentation and POS tagging task.
%Second, we describe the structure of the neural network.
%Finally, we focus on the feature of the neural network because the contribution of our research is feature of neural network. 
%A summary of feature for neural network is shown in Table 1. %~\ref{summary}    何かがおかしくインデックスが表示されない

%\subsection{Task of Word Segmentation and POS tagging} 
%{\bf Word Segmentation.}
%In this research, Japanese word segmentation is performed by pipeline processing.
%First, word segmentation is performed, and then POS is performed on the output.
%Both of these two tasks can be regarded as a sequence labeling problem~\cite{kudo-yamamoto-matsumoto:2004:EMNLP,nakagawa2004chinese}.
%In addition, there are many common parts in the information required as input between the two tasks.
%One of the popular methods of JMA is to formulate the task as a sequence labeling problem~\cite{kudo-yamamoto-matsumoto:2004:EMNLP,nakagawa2004chinese}.
Machine learning-based approaches for word segmentation build a classifier from an annotated corpus to classify whether word boundaries exist around a target character. 
In word segmentation, each character is assigned to several labels such as \{B, I, E, S\}, \{B, I, E\}, and \{B, I\} to indicate the segmentation. \{B, I, E, S\} represent {\it Begin}, {\it Inside}, {\it End}, and {\it Single}, respectively. 

%In POS tagging, each word is labeled with part of speech such as noun, verb, adjective.


%In the most simple label set \{B, I\}, ``B'' represents there is a word boundary to the left of the character and ``I'' represents there is no word boundary to the left of the character. \{E, M\} can be regarded as an extension to ``I'', in a sense that they indicate there is no word boundary to the left of the character, but they represent whether there is a word boundary to the right of the character or not, respectively.  ``S'' can also be regarded as an extension to ``B'' indicating there exist word boundaries both to the left and right to the target character.  
Classification of these labels is performed by running Viterbi algorithm over a word lattice~\cite{kudo-yamamoto-matsumoto:2004:EMNLP,nakagawa2004chinese,kaji-kitsuregawa:2013:IJCNLP} or by independently performing predictions (Neubig et al., 2011). However, the previous approaches use feature templates to expand window-based local features and suffer from data sparseness and lack of global information in a sentence. 

In order to address this problem, character-based embeddings and Long Short Term Memory (LSTM) network are proposed.
% that is known to work well in incorporating long dependency in NLP \cite{luong-pham-manning:2015:EMNLP,dai2015semi}. 
%\subsection{LSTM Network} 
 %In this section, we describe an LSTM Network for JWS. 
Figure \ref{ws_arch} shows an overview of the proposed framework. The model is similar to previous studies on CWS (Chen et al., 2015) although it incorporates character-based N-gram embeddings and a word dictionary sparse feature. 
%The first two serve as baselines of our proposed method in that feed forward neural network do not take into consideration any information outside the local window and recurrent neural network fails to accurately learn long history.
 
 \begin{figure*}[t]    
 \begin{center}    
 \includegraphics[bb=0 0 60 35,width=140mm]{ws_arch.png}              
 \caption{An overview of the proposed LSTM for Japanese word segmentation.}    
 \label{ws_arch}    
\end{center}    
\end{figure*}


 
%\subsection{Feed-Forward Neural Network} GGGGGGGGGGAAAAAAAAAAANNNNNNNNNNNBBBBBBBBBBBAAAAAAAAAAAAAARRRRRRRRRRRRRREEEEEEEEEEEEEEEEEE
%The most common sequence labeling approach is based on a local window \cite{kudo-yamamoto-matsumoto:2004:EMNLP,neubig-nakata-mori:2011:ACL-HLT2011}. 
%The window approach assumes that the label of a character depends on its neighboring characters.
%Given an input sentence, a window of size $k$ is shifted over the sentence from character $c_{1}$ to $c_{n}$, where $n$ is the length of the sentence.
%For each character $c_{t}$, the context characters $(c_{t-2}, c_{t-1}, c_{t}, c_{t+1}, c_{t+2})$ are fed into the classifier when the window size is 5. 
%Furthermore, start and terminal symbols are appended to a sentence as pre-processing. 
%The classifier learns weights for features created from the context characters using feature templates, and classifies the label of a character using the learned weights during decoding.

%However, previous approaches using feature templates suffer from data sparseness and overfitting. 
%They typically need to filter out infrequent features using certain threshold~\cite{takahasi-mori:2015:EMNLP} and/or employ feature selection such as L1 and L2 regularization~\cite{kudo-yamamoto-matsumoto:2004:EMNLP,neubig-nakata-mori:2011:ACL-HLT2011}. To address the data sparseness problem, we propose to use character embeddings we will describe in Section \ref{feature} (Figure \ref{architecture}).

In the neural architecture, character based embeddings for context characters are extracted by the lookup table layer and concatenated into a single vector $\bm{x_t}$$\in$ $\mathbb{R}^{H_{1}}$, where $H_{1}$ is the size of input layer. 
Then $\bm{x_{t}}$ is passed into the next layer that performs linear transformation $\bm{W_{1}}$  followed by an element-wise activation function $g$ such as sigmoid and $\tanh$ functions:
%\cite{collobert:2008}. 
%ここで，${H_{1}}$は（a）の入力文字列の embedding 層のサイズであり，その値は，$k × d$である．また，$d $は各文字のembedding の次元数である．


\begin{equation}
\label{ffnn_h}
 \bm{h_{t}} = g(\bm{W_{1}}\bm{x_{t}} + \bm{b_{1}})
 \vspace{6pt}
\end{equation}
where $\bm{W_1}$$\in$ $\mathbb{R}^{H_{2} \times H_{1}}$, $\bm{b_{1}}\in\mathbb{R}^{H_{2}}$, and $\bm{h_{t}}\in\mathbb{R}^{H_{2}}$. 
Additionally, $H_{2}$ denotes a hyper-parameter which indicates the number of hidden units in the hidden layer, $\bm{b_{1}}$ denotes a bias vector, and $\bm{h_{t}}$ denotes the resulting hidden vector. The final output is obtained by performing a softmax function after a similar linear transformation $\bm{W_2}$ to the hidden vector as follows:

\begin{equation}
 \bm{y_{t}} = \mathit{softmax}(\bm{W_{2}}\bm{h_{t}} + \bm{b_{2}})
 \vspace{6pt}
\end{equation}
where $\bm{W_2}$$\in$ $\mathbb{R}^{|T| \times H_{2}}$, $\bm{b_{2}}\in\mathbb{R}^{|T|}$, and $\bm{y_{t}}\in\mathbb{R}^{|T|}$. Thus, $\bm{b_{2}}$ denotes a bias vector, and $\bm{y_t}$  denotes the distribution vector for each possible label. In Japanese word segmentation, the most prevalent label set corresponds to \{B, I, E, S\} although the label sets do not significantly affect accuracy.


The RNN addresses the problem of lack of history by using recurrent hidden units in which output at each time is dependent on that of the previous time. 
%More formally, given a sequence $\bm{x_{t}}$, the RNN updates its recurrent hidden unit $\bm{h_{t}}$ by using the following function: 
%\begin{equation}
%\bm{h_{t}} = g(\bm{Uh_{t-1}} + \bm{W_{1}}\bm{x_{t}} + \bm{b_{1}}) 
%\end{equation}%
%where $\bm{U} \in \mathbb{R}^{H_2\times H_2}$ and $W_1 \in \mathbb{R}^{H_1 \times H_2}$ denote  linear transformations for the hidden units, $b_1 \in \mathbb{R}^{H_2}$ denotes a bias term, and $g$ denotes an element-wise activation function such as sigmoid and $\tanh$. Furthermore, $H_2$ denotes the number of hidden units in the hidden layer.
The recurrent neural network (RNN) is demonstrated as successful with respect to several NLP tasks such as language modeling~\cite{mikolov2010recurrent} and text generation~\cite{sutskever2011generating}. However, it fails to propagate dependencies over a long distance because of the vanishing and exploding gradient problem~\cite{hochreiter1997long}. 
%There are several Long Short Term Memory (LSTM). 

The LSTM provides addresses the problem by incorporating memory units to learn when to forget previous information and when to update memory cells given new information.
%Therefore, it is a natural choice to apply LSTM network to word segmentation task since the LSTM network can learn from data with long range temporal dependencies (memory) due to the considerable time lag between the inputs and their corresponding outputs.
%In addition, the LSTM has been applied successfully in many NLP tasks, such as sentiment analysis~\cite{wang-EtAl:2015:ACL-IJCNLP2} and machine translation~\cite{sutskever2014sequence}.
%The core of the LSTM model is a memory cell $c$ encoding memory at every time step of what inputs have been observed up to this step. 
%The behavior of memory cell $c$ is controlled by three gates called input gate $i$, forget gate $f$ and output gate $o$. 
%The definitions of the gates, a memory cell, output and hidden vectors are as follows:
%More formally, given a sequence $\bm{x_t}$, LSTM updates its recurrent hidden unit $\bm{h_t}$ by the following functions:

%\begin{eqnarray}
%\bm{i_{t}} &=& \sigma(\bm{W_{ix}}\bm{x_{t}} +\bm{W_{ih}h_{t-1}}) \\
%\bm{f_{t}} &=& \sigma(\bm{W_{fx}}\bm{x_{t}} +\bm{W_{fh}h_{t-1}}) \\
%\bm{c_{t}}& =&  \bm{f_{t}} \odot \bm{c_{t}} + \bm{i_{t}} \odot  \phi (\bm{W_{cx}x_{t}} + \bm{W_{ch}h_{t-1}}) \\
%\bm{o_{t}} &=& \sigma(\bm{W_{ox}}\bm{x_{t}} +\bm{W_{oh}h_{t-1}}) \\
%\bm{h_{t}} &=& \bm{o_{t}} \odot \phi(\bm{c_{t}})
%\end{eqnarray}

%\noindent
%where $\sigma$ and $\phi$ are the sigmoid and $\tanh$ functions, respectively.
%All the vectors have the same size as the hidden vector.
%The parameter matrices $\bm{W}$ with different subscripts are all square matrices, where $\bm{W_{ic}}$, $\bm{W_{fc}}$ and $\bm{W_{oc}}$ are diagonal matrices. $\odot$ denotes the element-wise product of the vectors.


\subsection{Character-Level Features}
This section discusses character-level features. As shown in Table 1, Character embedding and Character type embedding, and their N-gram for WS are introduced. The character vector $\bm{c_{t}}$ for JWS is described. Formally, the character vector $\bm{c_{t}}$ is defined as follows:

\begin{equation}
\bm{c_{t}} = \bm{l_{t}} \oplus \bm{e_{t}}
\vspace{6pt}
\end{equation}
where $\oplus$ denotes concatenation of the vectors, and $\bm{l_{t}} $ and $ \bm{e_{t}}$ denote  character embeddings and character type embeddings, respectively. These embeddings are fed to the input layer. Three features that are frequently used in JWS are discussed, and their realization as embeddings in the proposed architecture is described.

\subsubsection{Character Embeddings}

%The first step of using neural network to process symbolic data is to represent them into distributed vectors, also called embeddings~\cite{Bengio:2003:NPL:944919.944966,collobert:2008}.

In a word segmentation task, a character dictionary $C$ of size $|C|$ is created. 
%Unless otherwise specified, the character dictionary is extracted from the training set and unknown characters are mapped to a special symbol that is not used elsewhere. 
Traditional machine-learning approaches that use feature templates treat each character independently as a one-hot vector.
However, it is natural for a neural network model to represent discrete data as distributed vectors termed as embeddings~\cite{Bengio:2003:NPL:944919.944966,collobert:2008}. Representation learning is an actively studied topic in NLP because it overcomes the data sparseness problem. Thus, the same practice is followed to represent each character as a real-valued vector $v_{c}$ $\in$ $\mathbb{R}^{d}$ where $d$ is the dimensionality of the vector space. 
%The character embeddings are then concatenated to an input vector $x_{t}$ $\in$ $\mathbb{R}^{d}$.
With respect to each character, the corresponding character embedding $v_{c}$ is selected by a lookup table. 

%Here, the lookup table layer can be regarded as a simple projection layer where the character embedding for each context character is obtained by table lookup operation according to its index.

\subsubsection{Character Type Embeddings}
Character embeddings are extremely effective in identifying prefixes and postfixes. However, they could be too sparse while crossing a word boundary. In order to address this problem, it is helpful to exploit character types, such as hiragana, katakana, and kanji (e.g., ひらがな，カタカナ，漢字), for Japanese word segmentation~\cite{neubig-nakata-mori:2011:ACL-HLT2011}. For example, katakana sequence tends to correspond to a loan word, and a transition from a character type to another is likely to correspond to a word boundary \cite{nagata1999part}. 


%The one-hot vector is then  converted to character type embeddings as well as character embeddings. The embeddings are also concatenated to input vector of our neural network.   

\subsubsection{Character-Based N-gram Embeddings}
In addition to character type, the N-gram is effective in JWS \cite{neubig-nakata-mori:2011:ACL-HLT2011}.
Thus, character type sequence information is incorporated as embeddings. 
Each character is converted to a one-hot vector
%$\bm{ct}$ $\in$ $\mathbb{R}^{|ct|}$ 
corresponding to its character type. A one-hot vector is composed of either hiragana, katakana, kanji, alphabet, number, symbol, start symbol and terminal symbol. 
The advantages of a deep neural network include dealing with a sparse vector by converting it to a dense vector. 
This enables the utilization of a sparse feature such as character tri-gram. 
Additionally, a character-based N-gram is effective for sentence similarity and part-of-speech tagging \cite{wieting-EtAl:2016:EMNLP2016} and for Japanese morphological analysis~\cite{neubig-nakata-mori:2011:ACL-HLT2011}. Therefore, N-gram is used for character and character type embeddings.
More precisely, a one-hot vector is created for each uni-gram as well as for each bi-gram and tri-gram.
Each embedding is selected by a lookup table as well as uni-gram embeddings.

The embedding vectors $\bm{l_{t}}$ and $\bm{e_{t}}$ are defined as follows:

\begin{eqnarray}
\bm{l_{t}} &=& \bm{l_{[t-2:t]}} \oplus \bm{l_{[t-1:t]}} \oplus \bm{l_{[t]}} \\
\bm{e_{t}} &=& \bm{e_{[t-2:t]}} \oplus \bm{e_{[t-1:t]}} \oplus \bm{e_{[t]}} 
\vspace{6pt}
\end{eqnarray}
where $\bm{l_{[a:b]}}$ denotes the embedding for the strings from a to b. The same holds for $\bm{e_t}$.
%For example, $\bm{l_{[t-2:t]}}$ represents the embeddings for the string from

%Also, when N is 4, $\bm{ce^{N}_{t}}$ is follows: 
%\begin{equation}
%\bm{ce^{N}_{t}} = \bm{ct_{t-3}ct_{t-2}ct_{t-1}ct_{t}}
%\end{equation}
%The same is true of $\bm{cte^{N}_t}$.

%DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD
%DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD
\subsection{Word Dictionary}
%In this section, we explain word-level feature. As shown in Table 1, we describe Word dictionary for WS, Word/POS dictionary, Word uni-gram, and Word character type for POS.


\begin{figure*}[t]    
 \begin{center}    
 \includegraphics[bb=0 0 60 30,width=130mm]{dictionaryfeature.png}              
 \caption{Example of a dictionary vector.}    
 \label{dictionaryfeature}    
\end{center}    
\end{figure*}

Character embeddings, character type embeddings, and their N-gram extensions perform an excellent job with respect to learning character-based features from an annotated corpus. However, character-based JWS models lack word-level information that is useful to determine the character sequences constituting a word.
Thus, a Japanese morphological analyzer typically uses a dictionary.
It is essential for JWS that uses a word lattice during decoding to use word-level information such as a unigram and a bigram although this is not necessary for character-based JWS approaches.

However, it is not trivial to encode dictionary information into a neural network architecture.
%\new
\newcite{tsuboi:2014:EMNLP2014} suggests that it is not effective to learn both dense continuous and sparse discrete vector representations in the same layer. 
Thus, \newcite{tsuboi:2014:EMNLP2014}  is adopted to create a sparse dictionary vector although it is used for the input to the final output layer as shown in Figure \ref{ws_arch} instead of learning embeddings.


%An illustration of dictionary feature is in 
Figure \ref{dictionaryfeature} illustrates the creation of a dictionary vector.
The dictionary vector is composed of three parts as follows: left side feature $L$, right side feature $R$, and inside feature $I$. For example, L2 is activated if a word with a length corresponding to 2 exists in the dictionary on the left side of the prediction point. If the length of the word exceeds a certain threshold, the word length is cut off with respect to the length. In the study, 4 is adopted as the threshold following~\newcite{neubig-nakata-mori:2011:ACL-HLT2011}.
%We thus make the feature $R$ vector as the feature $L$. 
In contrast to $L$ and $R$, 
%$I$ is slightly special. In $L$ and $R$, the start point of a word candidate are restricted but $I$ is not so. 
$I2$ is fired if there exists a word that spans across the boundary and possesses length 2. It should be noted that $I$ is activated only if the length of the word exceeds 1  based on its definition.
%The other way of making dictionary feature is same as $L$ and $R$. 
Finally, the feature vectors are concatenated to a single vector termed as a dictionary vector $\bm{d_{t}}$.

The dictionary vector $\bm{d_{t}}$ is concatenated to the current hidden vector. 
%It is because it is not effective to convert dense vector representation as feature embeddings as well as character embeddings \cite{tsuboi:2014:EMNLP2014}. 
It should be noted that the current hidden vector $\bm{h_{t}}$ is on top of the LSTM network.
Formally, the new hidden vector $\bm{h^{\prime}_{t}}$ is defined as follows:

\begin{equation}  
\bm{h^{\prime}_{t}} = \bm{h_{t}} \oplus \bm{d_{t}}
\vspace{6pt}
 \end{equation}


%\subsubsection{Word dictionary}
%TBD
%\subsubsection{Word uni-gram and Word character type}
%TBD




\subsection{Training}
In this study, a cross entropy error is adopted as a loss function. Given an output vector $\bm{y_{t}}$, the loss to a correct distribution corresponding to $\bm{c_{t}}$ is computed as follows:

\begin{equation} 
 \bm{loss} = \sum_{t} -\bm{i_t} \log \bm{y_t} +  \frac{1}{2}\lambda \|\theta\|_{2}^2
 \vspace{6pt}
 \end{equation}



\noindent
where $\bm{i_{t}}$ denotes correct label distribution, $\lambda$ denotes a hyper parameter of L2 regularization, and $\theta$ indicates all parameters of the model. 
 
Following~\cite{socher2013parsing}, the diagonal variant of AdaGrad~\cite{duchi2011adaptive} with mini batches is used to minimize the objective. The update for the $i$-th parameter $\theta_{t,i}$ at time step $t$ is defined as follows:
\begin{equation}
\theta_{t,i} = \theta_{t-1,i} - \frac{\alpha }{\sum^{t}_{t=\tau} g^{2}_{\tau,i}}g_{t,i} 
\vspace{6pt}
\end{equation}

\noindent
where $\alpha$ denotes the initial learning rate, and $g_{\tau} \in \mathbb{R}^{|\theta_{i}|} $ denotes the subgradient at time step $\tau$ for parameter $\theta_{i}$. 







%EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
%EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
 
\section{Experiments}
\begin{figure}[t]    
 \begin{center}    
 \includegraphics[bb=0 0 60 43,width=80mm]{learning_curve.png}              
 \caption{Performance of the proposed method with respect to the BCCWJ development set.}    
 \label{learning_curve}    
\end{center}    
\end{figure}




The proposed neural word segmentation method is evaluated on several JWS corpora.
In order to evaluate neural network architectures, a feed forward network (FFNN) and a recurrent neural network (RNN) is prepared for JWS. The FFNN is illustrated by a dotted line in Figure \ref{ws_arch}. Additionally, RNN uses the same inputs as LSTM although it does not use any LSTM units.

The experiments are separated into two parts. First, neural network architectures and features are compared with previous state-of-the-art method on a balanced corpus (See top of Table \ref{state_result}). 
%Second, we evaluate our Japanese adaptation such as character type and dictionary feature and compare our method with state-of-the-art method (See bottom of Table \ref{state_result}). 
Second, the method is evaluated on a newspaper corpus annotated with a different segmentation criterion (See Table \ref{kc_result}). 
\subsection{Datasets}
\begin{table}[t]
                                                                                                                                                                                                                                                                                             
 \begin{center}                                                                                                                                                                                                                                                                                                             
\caption{The details of the corpora used in the experiments.}                                                                                                                                                                                                                                                                                 
\label{bccwj}                                                                                                                                                                                                                                                                                                              
 \begin{tabular}{p{45mm} rr}                                                                                                                                  
  \toprule                                                                                                                                                    
        \multicolumn{1}{c}{domain} & \multicolumn{1}{c}{train} &\multicolumn{1}{c}{test} \\                                                                                             
        \midrule                                                                                                                                                                                                                                                                                                            
    Yahoo! Japan Answers  &5,880&496 \\    
    Yahoo! Japan Blog&7,036&506\\                                                                                                                                  
    White paper&5,471 &496 \\                                                                 
    Magazine &12,369 &492  \\   
    Newspaper &16,222 &495\\                                                                                                                                
    Book& 9,470&499 \\
    BCCWJ All  &56,448 &2,984 \\  \hline
    KC All &18,455 & 1,234  \\                                                                                                                                                                                                                                                                                                                                                                   
    \bottomrule                                                                                                                                               
    \end{tabular}                                                                                                                                                                                                                                                                                                           
    \end{center}                                                                                                                                                                                                                                                                                                            
 \end{table}  

The methods are evaluated with respect to two different datasets, namely a popular Japanese corpus BCCWJ (Balanced Corpus of Contemporary Written Japanese)~\cite{maekawa2014balanced} and another widely used Japanese corpus Kyoto University Corpus version 4.0 (KC). 
The BCCWJ is composed of various domains while KC only includes the newswire domain. The details of the corpora are shown in Table \ref{bccwj}.  
The train and test split of BCCWJ are followed according to the Project Next NLP\footnote{\url{http://plata.ar.media.kyoto-u.ac.jp/mori/research/topics/PST/NextNLP.html}}. 
The same train and test split of KC used in previous studies~\cite{kudo-yamamoto-matsumoto:2004:EMNLP,uchimoto2001unknown} were adopted.

With respect to word-level features,
% \new
\newcite{neubig-nakata-mori:2011:ACL-HLT2011} do not use any external dictionary but use the dictionary created from the training corpus. Hence, the same scenario is adopted, and all the words in training corpus are added although singletons are omitted to prevent overfitting on the training data as described in \cite{neubig-nakata-mori:2011:ACL-HLT2011}.
In order to analyze the effect of the dictionary feature, a larger dictionary created from both training and test sets is recreated. This is termed as gold dict in Table \ref{state_result}.


\subsection{Tools}
In the experiments, a popular JWS tool KyTea (ver.0.4.6) \footnote{\url{http://www.phontron.com/kytea/index.html}} is used that implements \cite{neubig-nakata-mori:2011:ACL-HLT2011}. A KyTea model is trained by the provided scripts for training. It internally creates a dictionary as described above. Pre-trained KyTea models adopt their own word segmentation criterion extended from that of BCCWJ, and thus KyTea models are re-trained to ensure a fair comparison.
%a model of KyTea in this work is trained by only training data so the output of our experiments follows a criterion of training data.

Additionally, neural network-based JWS models including FFNN, RNN, and LSTM are implemented by using a neural network framework termed as Chainer (ver 1.4.0)\footnote{\url{http://chainer.org}}\cite{chainerlearningsys2015}. 

\subsection{Hyper Parameters}
Several parameter combinations inspired by previous studies~\cite{chen-EtAl:2015:EMNLP2} are investigated in the preliminary experiments. The complete set of parameters used in the study is shown in Table \ref{parameter}. The BCCWJ development set is used for tuning hyper parameters. Based on Figure \ref{learning_curve}, it is observed that the proposed method is very fast to learn although a CPU is used. The duration corresponds to 24 h per epoch. 

\noindent
{\bf Window size.} In the preliminary experiments, the results indicate that window size 5 is better than others in terms of both accuracy or time. Window size 7 did not significantly improve the F1 score from window size 5, and thus window size 5 is selected due to the trade-off between accuracy and time.

\noindent
{\bf Dimension of character type embeddings.} The dimension of character embeddings is fixed by following (Chen et al., 2015b). However, six dimensions of character type embeddings are searched. 
The dimension of character type embeddings is selected as corresponding to 10 because 10 yields a better performance than others with respect to both accuracy or time as indicated by the preliminary experiments. Although window sizes 20 and 50 are competitive with respect to window size 10, window size 10 is selected given the time complexity.

\noindent
{\bf Label set.}  In CWS, the label set \{B, I, E, S\} is often used. In contrast, various label sets are adopted in JWS. Three label sets are explored, and the findings suggest that \{B, I, E, S\} is slightly better than the others.

\noindent
{\bf Learning rate.} In this task, the learning rate largely affects accuracy. Although a learning rate of 0.1 appears to exceed those in other NLP tasks, a small learning rate (such as 0.01) degrades accuracy and significantly affects learning time. Thus, the learning rate of 0.1 is selected for all the experiments.

%As over all impression, in this task, the size of hyper parameter is better too big than too small but too big hyper parameter is time-consuming.

\begin{table}[t]
                                                                                                                                                                                                                                                                                                                                                                                                                                 
 \begin{center}                                                                                                                                                                                                                                                                                                             
\caption{The Hyper parameter set of the study.}                                                                                                                                                                                                                                                                                  
\label{parameter}                                                                                                                                                                                                                                                                                                           
 \begin{tabular}{p{50mm}| r }
  \toprule
    \multicolumn{1}{c|}{hyper parameter} & \multicolumn{1}{c}{value} \\
    \midrule                                                                                                                                             
    window size & 5 \\                                                                                                                                        
    character embeddings &  100\\                                                                                                                           
    character type embeddings  &  10\\                                                                                                                          
    hidden layer size &  150\\                                                                                                                                     
    label set& \{B, I, E, S\} \\                                                                                                                                      
    learning rate  &0.1\\                                                                                                                                           
    %  & η=0.2\\ \hline                                                                                                                                       
    coefficient of L2  regularization & 0.0001\\ %正則化項の係数 & 0.0001\\                                                                                                                              
   % Dropout rate （入力層のおける）& p = \\ \hline
   \bottomrule
     \end{tabular}                                                                                                                                                                                                                                                                                                           
    \end{center}                                                                                                                                                                                                                                                                                                            
 \end{table}
 
                  
\subsection{Results}
Tables \ref{state_result} and \ref{kc_result} show the experimental results for the BCCWJ Corpus and Kyoto Corpus. In both corpora, the LSTM-based method outperformed the state-of-the-art method \cite{neubig-nakata-mori:2011:ACL-HLT2011}. Table \ref{error} illustrates the performance of the two methods per domain breakdown. The accuracy of the proposed method in terms of token-level F1 and sentence-level accuracy exceeded those of others in four domains out of six, and this resulted in improvements in the overall performance.


\section{Discussion}
\paragraph{Models.}
Table \ref{state_result} shows that LSTM is superior to FFNN and RNN by using the same feature set (character embeddings only). It demonstrates the effectiveness of modeling a context by LSTM.

\noindent
{\bf Character type.} Comparing {\it LSTM} with {\it LSTM + character type}, F1 improves by 0.25 points. The result shows that character type embeddings are useful in JWS. However, the advantage of using character type embeddings is smaller when compared to that when the model is changed to LSTM. 
This suggests that the choice of architecture has a greater effect on the final accuracy.

\noindent
{\bf Dictionary feature.} The addition of a dictionary feature to {\it LSTM + character type} improves F1 by 0.37. This result shows that dictionary feature is effective in JWS. However, the addition of the dictionary feature to {\it LSTM + character type + N-gram} does not result in any notable difference. It is assumed that character based N-gram embeddings subsume the dictionary feature because the dictionary is created from the training corpus. 
Additional experiments using the gold dictionary created from the test corpus support this hypothesis\footnote{It should be noted that the singletons in the combined corpus are removed while creating the gold dictionary, and thus the test corpus may still contain words that are not in the gold dictionary.}. 

\noindent
{\bf N-gram embedding.} A comparison of {\it LSTM + character type} with {\it LSTM + character type + N--gram} indicates N-gram embeddings significantly improve the performance of the model by a large margin. Traditional machine learning-based approaches, such as CRF and SVM, do not use the advantage of the sparse features while the LSTM-based proposed model successfully exploits this information.

 \begin{table}[t]     
 
                                                                                                                                                                                                                                                                                                 
 \begin{center}                                                                                                                                                                                                                                                                                                             
\caption{Experimental results of Japanese word segmentation on BCCWJ.}                                                                                                                                                                                                                                                                                 
\label{state_result}                                                                                                                                                                                                                                                                                                              
 \begin{tabular}{p{64mm}| r}                                                                                                                                  
  \toprule                                                                                                                                                    
        \multicolumn{1}{c|}{Methods} & \multicolumn{1}{c}{F1}\\                                                                                             
        \midrule                  
      FFNN  & 96.53 \\                                                                                                                                     
      RNN  &  96.46 \\                                                                                                                               
      LSTM  & 97.00 \\                                                                                                                                                                                                                                                                                          
      LSTM + char type  & 97.25 \\    
      LSTM + char type + dict & 97.37 \\                                                                                                                             
     % LSTM + character type + 1-2gram &  98.05 \\\hline
      LSTM + char type + N-gram & 98.41\\ 
    %  LSTM + character type + 1-4gram & ***\\ \hline
      LSTM + char type + N-gram + dict&{\bf 98.42} \\ 
      LSTM + char type + N-gram + dict (gold)& 98.67 \\  \hline
      KyTea 0.4.6 & 98.34 \\                                                                                                                                                                                                                                                                                                                   
    \bottomrule                                                                                                                                               
    \end{tabular}                                                                                                                                                                                                                                                                                                           
    \end{center}                                                                                                                                                                                                                                                                                                            
 \end{table} 
 
 
\begin{table}[t]
                                                                                                                                                                                                                                                                   
 \begin{center}                                                                                                                                                                                                                                                                                                             
\caption{Experimental results of Japanese word segmentation on Kyoto Corpus.}                                                                                                                                                                                                                                                                                 
\label{kc_result}                                                                                                                                                                                                                                                                                                              
 \begin{tabular}{p{60mm}| r }                                                                                                                                  
  \toprule                                                                                                                                                    
        \multicolumn{1}{c|}{Methods} & \multicolumn{1}{c}{F1}\ \\                                                                                             
        \midrule                                                                                                                                                                                                                                                                                                            
    LSTM + char type + N-gram + dict  & {\bf 96.47}\\  \hline                                                                                                                            
    KyTea 0.4.6 &  96.21\\ 
    \bottomrule                                                                                                                                               
    \end{tabular}                                                                                                                                                                                                                                                                                                           
    \end{center}                                                                                                                                                                                                                                                                                                            
 \end{table}  
 
\section{Error Analysis}

\begin{table*}[t]   
                                                                                                                                                                                                                                                                                                
 \begin{center}                                                                                                                                                                                                                                                                                                             
\caption{Token-level and sentence-level performance on various domains. The first term of the number of incorrect sentences indicates that both JWS predicted in correct results, and the second term indicates that only the corresponding method predicted an incorrect result. }                                                                                                                                                                                                                                                                                 
\label{error}                                                                                                                                                                                                                                                                                                              
 \begin{tabular}{p{53mm}|r|r|r|r}                                                                                                                                  
  \toprule                                                                                                                                                    
        \multicolumn{1}{c|}{Domain} & \multicolumn{2}{c|}{KyTea 0.4.6} & \multicolumn{2}{c}{This work} \\   \cline{2-5}
        \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c|}{\# of incorrect sent.} & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c}{\# of incorrect sent.} \\   
         %\multicolumn{1}{c|}{domain} & \multicolumn{1}{c|}{F1} &  \multicolumn{1}{c|}{# of incorrect sent.} & \multicolumn{1}{c|}{F1} &  \multicolumn{1}{c}{# of incorrect sent.}  \\                                                                                             
        \midrule                                                                                                                                                                                                                                                                                                            
      Yahoo! Japan Answers& 98.38 &50+25& {\bf 98.44} &  50+{\bf19}\\     
      Yahoo! Japan Blog&  {\bf 99.75}& 76+22& 99.73&76+{\bf21}\\                                                                                                                              
      White paper& {\bf 99.20} &60+{\bf21}  &99.08& 60+24\\
      Book&  98.15&63+{\bf19} &  {\bf 98.28}&63+28\\ 
      Magazine & 96.70& 73+29  &{\bf 97.25}&73+{\bf17} \\ 
       Newspaper&  98.19 &60+36&  {\bf 98.46}& 60+{\bf15}\\  \hline                                                  
      All  & 98.34 &382+152& {\bf 98.42} & 382+{\bf124}\\                                                                                                                                                                                                                                                                                                                                       
    \bottomrule                                                                                                                                               
    \end{tabular}                                                                                                                                                                                                                                                                                                           
    \end{center}                                                                                                                                                                                                                                                                                                            
 \end{table*} 

\begin{table*}[t] 
                                                                                                                                                                                                                                                                                                                                                
 \centering                                                                                                                                                                                                                                                                                                                                    
 \begin{center}                                                                                                                                                                                                                                                                                                                                
\caption{An example of the error in this study and KyTea. The character ``\textbar '' indicates the word boundary, and the bold face indicates the incorrect part.}                                                                                                                                                                                                                                                                           
 \label{error_analysis}                                                                                                                                                                                                                                                                                                                          
 \begin{tabular}{ l | l | l} \hline  
   \multicolumn{1}{c|}{Methods} & \multicolumn{1}{c|}{Example} &\multicolumn{1}{c}{correct/inccorect}\\         
 \toprule    
 %\midrule                                                                                                                                                                                                                                                                                                       
      This work & エルマー \textbar {\bf とりゅう} \textbar の \textbar 絵 \textbar で  & incorrect\\
     KyTea 0.4.6 &エルマー \textbar と \textbar りゅう \textbar の \textbar 絵 \textbar で  &correct\\ \hline
      This work &うち \textbar {\bf がまんま} \textbar その \textbar 環境 \textbar です \textbar 。&incorrect\\
      KyTea 0.4.6 & うち \textbar が \textbar まんま \textbar その \textbar 環境 \textbar です \textbar 。&correct\\ \hline
     This work & 七百 六十 \textbar 一 \textbar の \textbar ため池 \textbar など \textbar 被害&correct\\  
     KyTea 0.4.6 &  七百 \textbar 六十 \textbar 一 \textbar の \textbar {\bf ため \textbar 池} \textbar など \textbar 被害 &incorrect  \\ \hline
     This work &  思う \textbar と \textbar うんざり \textbar です \textbar ．&correct\\
     KyTea 0.4.6 & 思う \textbar {\bf とうんざり} \textbar です \textbar．&incorrect\\
     \bottomrule                                                                                                                                                                                                          
   \end{tabular}                                                                                                                                                                                                                                                                                                                               
    \end{center}                                                                                                                                                                                                                                                                                                                               
 \end{table*}


\subsection{Effect of Domain}
In order to determine the characteristics of the proposed method, an error analysis was conducted by comparing the proposed method with KyTea with respect to different domains. Thus, F1 was computed for each domain of BCCWJ, and the number of incorrect sentences was counted. Table \ref{error} summarizes token-level and sentence-level comparison between the proposed model and KyTea. 
The pairs that exhibited a large margin in F1 were selected. Thus, White paper, Magazine, and Book were analyzed).

{\bf White paper.} This domain comprises of official documents published by the government. Thus, kanji covers a large portion of the corpus. Additionally, the number of characters per sentence is high. In this domain, the proposed method is only inferior to 
\newcite{neubig-nakata-mori:2011:ACL-HLT2011} with respect to both F1 and the number of incorrect sentence, and this is potentially because a long sequence introduced noise to the LSTM-based models. 

{\bf Magazine.} This domain contains colloquial expressions as well as formal expressions.
Hiragana occupies a large portion of this corpus due to the colloquial expressions. 
Furthermore, F1 for this domain is the lowest in both methods. 
The results indicate that hiragana exhibits a poor performance. However, the proposed method is more robust than KyTea in this domain. This could be due to the modeling of contextual information since the hiragana sequence tends to fall outside the local window size. 

{\bf  Book.}  This domain typically includes named entities (NE) such as a company name. This corpus is balanced in terms of the proportion of character types.
Overall, the proposed model tends to be robust for compounds of different character type (e.g. Famiポート (Fami Port; multimedia vending machine)), whereas
\newcite{neubig-nakata-mori:2011:ACL-HLT2011}'s model correctly classifies words composed of unique character type (e.g. ポストドクター (Postdoc)). It is considered that the difference between token-level and sentence-level accuracy highlights the characteristic of the methods. The proposed method typically produces less errors although it does not consistently perform word segmentation across the corpus.


\subsection{Example}
In order to investigate the characteristics of the proposed method from a different angle, actual examples of word segmentation are demonstrated. Table \ref {error_analysis} shows a comparison of four examples for the current study and KyTea 0.4.6.  


The proposed method possesses two characteristics. The first characteristic is that strings with the same character type tend to form a word unit. This characteristic is demonstrated by the first and second examples. 
In the first example, \mbox{``と (and)''} and \mbox{``りゅう (dragon)''} are different words although they possess the same character type ``Hiragana'', and thus they are incorrectly combined to form a fake word. In the second example, mbox{``が (NOM)'' }and \mbox{``まんま (just)''} are also incorrectly connected. This type of error tends to occur when the character type corresponds to \mbox{``Hiragana,''} that includes many high-frequency ambiguous single-character particles.
 
Another characteristic of this method is that words including different character types tend to be broken by KyTea at the position where a character type is changed. This characteristic is demonstrated by the third example. In the third example, \mbox{``ため池 (storage reservoir)''} is a single word that consists of \mbox{``ため (storage)''} and \mbox{``池 (reservoir)''} although KyTea fails to recognize the word since \mbox{``ため''} and \mbox{``池''} possess different character types. In contrast, the proposed method correctly identifies the word.

However, there are cases where contrary results are indicated. In the fourth example, \mbox{``と (and)''} and \mbox{``うんざり (fed up)''} correspond to different words and possess the same character type \mbox{``Hiragana''}. An analysis of the first and second examples indicates that the proposed method tends to form a fake word that comprises of the same character type although it yields a correct segmentation result. Thus, the qualitative analysis is considered difficult.




%Char type embedding (adaptation 1) The information of character type embeddings improves F1. The way to treat it seems to suit in our task. 
%Dictionary embedding (adaptation 2) The information of dictionary is not effective in our task. Some reasons we consider are bellow:
%N-gram embeddings seems to be enough information in our task since the dictionary is made from training corpus.
%The way to make use of dictionary doesn’t seem to be appropriate.
%N-gram embeddings The n-gram embeddings drastically improve F1.
%The result indicates that neural architecture is useful for dealing with sparse feature.




\section{Related Works}
\label{sec:relatedworks}
%We expand one of the state-of-the-art method in Japanese word 
%segmentation (Neubig et al., 2011). They propose a pointwise
%prediction method based on sequence labeling approach.

%In Japanese, word segmentation are often performed jointly as Japanese word segmentation (JWS). 
In JWS, a supervised learning approach is widely used. A popular method in JWS involves creating a word lattice by using a dictionary and using Viterbi decoding~\cite{kudo-yamamoto-matsumoto:2004:EMNLP,sassano2002empirical}. This approach is known to yield accurate results by considering the sequence of words although it is not robust if training data differs from test data. Another popular approach employs point-wise prediction by using a local window~\cite{neubig-nakata-mori:2011:ACL-HLT2011,NEUBIG10.408}. However, both approaches do not consider the global context because they use feature templates of a fixed length. Additionally, they both suffer from feature sparseness.

%Recently, the study analyzing web text such as Twitter have been actively researched. In processing of web text, colloquial expression, non-standard expression, and new word are often used. These expressions can cause error. Contrary, for web text, there is unsupervised method utilizing large amount of unlabeled data\cite{mochihashi-yamada-ueda:2009:ACLIJCNLP,uchiumi-tsukahara-mochihashi:2015:ACL-IJCNLP}.
%Moreover, there is supervising method using normalizing rule of character and word base and constructing normalization corpus~\cite{saito-EtAl:2014:Coling,sasano-kurohashi-okumura:2013:IJCNLP,kaji-kitsuregawa:2014:EMNLP2014}. The method based on unsupervised learning is low cost but has a problem to adjust the criterion of word segmentation to human annotation. Whereas, the method based on supervised learning can adapt to the known rule of normalization but is required to add new word and new normalization rule for new word and the unknown rule of normalization. 

Recently, deep neural network architectures have been widely studied in the CWS task ~\cite{chen-EtAl:2015:EMNLP2,chen-EtAl:2015:ACL-IJCNLP5,pei-ge-chang:2014:P14-1,zhang-zhang-fu:2016:P16-1,cai-zhao:2016:P16-1}. These approaches are mainly divided into two types: structured prediction mode~\cite{zhang-zhang-fu:2016:P16-1,cai-zhao:2016:P16-1} and pointwise prediction model ~\cite{chen-EtAl:2015:EMNLP2,chen-EtAl:2015:ACL-IJCNLP5,pei-ge-chang:2014:P14-1}. However, a deep neural network approach requires high computational cost when compared with previous approaches. 
In JWS, \newcite{morita-kawahara-kurohashi:2015:EMNLP} proposed integrating a recurrent neural network language model into JWS by interpolating it with traditional JWS. As opposed to using recurrent neural architecture as side information, word segmentation in Japanese is directly learned by using LSTM. 


Recently, a neural network approach for normalization was explored \cite{kann-cotterell-schutze:2016:EMNLP2016, ikeda2016norm} \newcite{kann-cotterell-schutze:2016:EMNLP2016} by using a character based encoder-decoder model, and state-of-the-art accuracy was achieved for the task of canonical
morphological segmentation.
The method is based on unsupervised learning, and thus it is learned at a low cost. However, it is necessary to adjust the criterion of word segmentation learned in an unsupervised manner to human annotation.




%This is because we extend neural network approach and evaluate neural network appraoch for JWS. 
 
%さらに，最近では，Twitter 等のウェブ文書の単語分割，形態素解析に対しての研究が活発である．ウェブ文書においては，口語的な表現，正規の表記ではない略語，新たに造られる新語などが多く存在し，解析の性能が落ちることが問題となっている．これに対し，生のコーパスのみを用いた教師なし学習による単語分割\cite{mochihashi-yamada-ueda:2009:ACLIJCNLP}や形態素解析\cite{uchiumi-tsukahara-mochihashi:2015:ACL-IJCNLP}，辞書追加，文字ベース，単語ベースの正規化規則を適用することによる教師あり学習による手法\cite{saito-EtAl:2014:Coling,sasano-kurohashi-okumura:2013:IJCNLP,kaji-kitsuregawa:2014:EMNLP2014}が存在する．教師なし学習による手法はコストが非常に小さいが，単語分割の基準を機械学習によって決めるため，意図した出力が難しくなる問題がある．教師あり学習による手法は，文字ベースの正規化により，コストを削減しているが，新しく造られる新語や規則による対処ができない形態素に対しては，辞書の拡張を行う必要がある．

%一方，中国語の単語分割において，深層ニューラルネットワークを用いた手法が盛んに研究されている．\cite{chen-EtAl:2015:EMNLP2,chen-EtAl:2015:ACL-IJCNLP5,pei-ge-chang:2014:P14-1}．深層ニューラルネットワークによる手法は学習に時間がかかるが，文字 embedding を利用した線形時間で学習できる高性能な単語分割器も存在する\cite{ma-hinrichs:2015:ACL-IJCNLP}．日本語形態素解析においても，深層学習を利用した研究はRNNLMを利用した形態素解析\cite{morita-kawahara-kurohashi:2015:EMNLP}があるが，中国語であるような，深層ニューラルネットワークを利用した単語分割の研究は存在しない．そこで，本研究では，日本語形態素解析のための深層ニューラルネットワークを利用した日本語単語分割について分析を行った．

%日本語のコーパスは，日本語書き言葉均衡コーパス\cite{maekawaBCCWJ2008}（以下，BCCWJ）が広く利用されており，書籍全般，雑誌全般，新聞，白書，ブログ， ネット掲示板，教科書，法律などのジャンルに対して，単語分割と，付加情報として，品詞，活用，基本形，読みなどがアノテーションされている．


%\cite{kudo-yamamoto-matsumoto:2004:EMNLP}
%\cite{neubig-nakata-mori:2011:ACL-HLT2011,NEUBIG10.408}
%\cite{chen-EtAl:2015:EMNLP2,chen-EtAl:2015:ACL-IJCNLP5,pei-ge-chang:2014:P14-1}．
%\cite{ma-hinrichs:2015:ACL-IJCNLP}．
%\cite{morita-kawahara-kurohashi:2015:EMNLP}

%POS tagging is solved simultaneously with word segmentation in Japanese morphological analysis~\cite{kudo-yamamoto-matsumoto:2004:EMNLP,sassano2002empirical}. 
%On the other hand, in Chinese, word segmentation is often solved alone~\cite{chen-EtAl:2015:EMNLP2,chen-EtAl:2015:ACL-IJCNLP5,pei-ge-chang:2014:P14-1,ma-hinrichs:2015:ACL-IJCNLP}.
%In Chinese, word segmentation using a neural network has been actively researched, but POS tagging has not actively been conducted.
%As in the study of Japanese morphological analysis of~\newcite{neubig-nakata-mori:2011:ACL-HLT2011,NEUBIG10.408}, the information necessary for word segmentation and POS tagging has many common parts.
%Therefore, in this research, we propose a method to solve both word segmentation and POS segmenting by neural network based on the research of~\newcite{neubig-nakata-mori:2011:ACL-HLT2011}.


%In languages such as English where words are separated by spaces, unlike Chinese and Japanese, research to solve POS tagging by using a neural network has been conducted since ancient times~\cite{nakamura1990pos,schmid1994pos,tsuboi:2014:EMNLP2014}.
%\newcite{schmid1994pos} reports that POS tagging by neural network improves better accuracy than the usual hidden Markov model (HMM).
%\newcite{tsuboi:2014:EMNLP2014} proposes a method to introduce into the corpus-wide information neural network, and records the accuracy of state-of-the-art in English  POS tagging task.
%Therefore, in this research, we propose a method to apply knowledge of these research to Japanese morpheme analysis which uses much information such as character type and dictionary.

\section{Conclusion}
%We combine Long Short-Term Memory (LSTM) for Word Segmentation (Chen et al., 2015), which has achieved state-of-the-art in Chinese Word Segmentation.
In this paper, an LSTM neural network approach to JWS was presented. Learning specific Japanese features, such as character type and character N-gram, as embeddings, and dictionary features as a sparse vector was proposed. The proposed method has been shown to achieve state-of-the-art accuracy on various domains. 
The empirical results of the study have suggested further opportunities to investigate continuous features not only for WS but also for POS tagging. 

In JWS, it is important to deal with colloquial expressions that are frequently found in dialogue-based conversations and web text~\cite{saito-EtAl:2014:Coling,sasano-kurohashi-okumura:2013:IJCNLP,kaji-kitsuregawa:2014:EMNLP2014}. It is expected that deep neural architectures, such as CNNs, may be effective in this scenario because of their ability to learn robust representations for characters and words~\cite{ling-EtAl:2015:EMNLP2}. 

%Previous work in Chinese work segmentation (CWS) have succeeded in using neural network and the model achieve state-of-the-art accuracy. Unlike Chinese, Japanese has various character type such as hiragara, katakana, and kanji. Besides, traditional Japanese word segmentation utilize word dictionary. For this situation, we propose a solution to incorporate these information. Ex- perimental results show that our proposed model outperformed state-of-the-art method. Since we first introduce neural network approach for JWS, we investigate some neural network architec- ture and analyze the characteristic by comparing with current state-of-the-art method on several corpora that have different nature.

%As a result, although neural network is competitive with the state-of-the-art method, it is not effective to analyze colloquial expressions. 
%To deal with colloquial expressions, we will investigate several ways for more robust neural word segmentation in terms of neural network architecture and resource usage.
%The architecture and resource usage we will try are bellow:

%\section*{Acknowledgments}

%Do not number the acknowledgment section.

%\bibliography{ws}
%\bibliographystyle{ijcai17}

\bibliography{ijcnlp2017}
\bibliographystyle{ijcnlp2017}

\end{document}

